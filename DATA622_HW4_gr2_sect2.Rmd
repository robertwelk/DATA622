---
title: "DATA 622 HW3"
author: "Robert Welk"
date: "10/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
*For this assignment, we will be working with a very interesting mental health dataset from a real-life research project. All identifying information, of course, has been removed. The attached spreadsheet has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as creative as you want. The assignment is designed to really get you to think about how you could use different methods.* 

**Data Dictionary**
C:  Sex:  Male-1, Female-2
D: Race:  White-1, African American-2, Hispanic-3, Asian-4, Native American-5, Other or missing data -6
E - W ADHD self-report scale: Never-0, rarely-1, sometimes-2, often-3, very often-4
X – AM Mood disorder questions: No-0, yes-1; question 3: no problem-0, minor-1, moderate-2, serious-3
AN – AS  Individual substances misuse:  no use-0, use-1, abuse-2, dependence-3
AT Court Order:  No-0, Yes-1
AU Education: 1-12 grade, 13+ college
AV History of Violence: No-0, Yes-1
AW Disorderly Conduct: No-0, Yes-1
AX Suicide attempt: No-0, Yes-1
AY Abuse Hx: No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7
AZ Non-substance-related Dx: 0 – none; 1 – one; 2 – More than one
BA Substance-related Dx: 0 – none; 1 – one Substance-related; 2 – two; 3 – three or more
BB Psychiatric Meds: 0 – none; 1 – one psychotropic med; 2 – more than one psychotropic med

R Packages
```{r messge=F, warning=F}
library(DataExplorer)
library(tidyverse)
library(corrplot)
library(scales)
library(mice)
library(caret)
library(GGally)
library(viridis)
library(plotly)
```

Import data
```{r}
setwd('C:/Users/robbj/OneDrive/Desktop/DATA622')
df <- read.csv('ADHD_data.csv', na.string="") %>% as_tibble()
```


# 1. EDA 
*Conduct a thorough Exploratory Data Analysis (EDA) to understand the dataset. (20 points)*


## 0. Overview of data
- Initial is an identifier, and will have no value in supervised or unsupervised learning
- missing values will be imputed 
```{r}
# names 
str(df)

# missing values
plot_missing(df) 
```

## 1. Demographics
*Age, Sex, Race, and Education*
- wide range of ages
- a High School degree is the most common educations level. More participants had not completed high school than had college education
- African Americans were the most common followed by White. Other ethnicities had only a few participants
- there were more males than females 
- stratified sample: there are no patterns in subgroups 

```{r}
# variables that can be categorized as demographics
demographics <- c('Age', 'Race', 'Sex', 'Education')

# summary of demographics
df %>% dplyr::select(all_of(demographics)) %>%  summary() 
df %>% dplyr::select(all_of(demographics)) %>%  str

# get counts of each group 
df %>% 
  dplyr::select(all_of(demographics), Initial) %>% 
  pivot_longer(-Initial) %>% 
  ggplot(aes(x=value)) + 
  geom_bar(stat="count") + 
  facet_wrap(~name, scales="free") + 
  theme_minimal()

# maybe but some groups and make boxplots
df %>% 
  filter(Race %in% c(1,2)) %>% 
  dplyr::select(Age, Sex, Race, Education,Initial) %>% 
  ggplot(aes(x=Age,
             y=Education,
             col=factor(Sex),
             shape=factor(Race))) + 
    geom_point(size=3) + 
    scale_color_viridis_d() + 
    theme_minimal() + facet_wrap(~Race)
             
# change Race and Sex to factors
df <- df %>% update_columns(c("Sex","Race"), as.factor) 
# levels(df$Sex) <- c("Male","Female")
# levels(df$Race) <- c("White","Black", "Hispanic","Other")
# range(df$Education, na.rm=T)

# new variables 
df$EDUCATION <- cut(df$Education, breaks=c(9, 12, 13, 19), include.lowest = T)
levels(df$EDUCATION) <- c("NoDiploma", "HS_Diploma","College")

# Replace 2 (female) to 0 so that range is from 0 to 1
df$Sex[df$Sex==2] <- 0

```

## 2. ADHD self-report
- from the correlation plot, ADHD.TOTAL captures most of the information of the individual questions. The total value of the can solely be used instead of each the questions with minimal information loss.
```{r}
# 
# df %>% 
#   dplyr::select(starts_with("ADHD")) %>% 
#   pivot_longer(-ADHD.Total) %>% 
#   ggplot(aes(x=factor(name),y=value)) + 
#   geom_boxplot()+ 
#   geom_point(position="jitter",alpha=.2)




# Q6 is somewhat different than the rest
na.omit(df) %>% 
  select(starts_with('ADHD')) %>% 
  cor () %>%  
  corrplot(method='color', diag=F, type='lower')

# find the mean self-reported score
hist(df$ADHD.Total)
summary(df$ADHD.Total/18)
df %>% ggplot(aes(ADHD.Total))+geom_histogram(col="black", alpha=.5)
```

## 3. Mood Disorder self-report
- Mood disorder self-report is also summarized by the total score of the individual questions. The single variable can be used instead of all the questions since information is repeated.
- There is some correlation (~0.41) between ADHD and mood disorder
- There is not much correlation between self-report and demographics except for education level
```{r}
# MD.TOTAL captures most of the information of the individual questions
# Q6 is somewhat different than the rest
na.omit(df) %>% 
  select(starts_with('MD')) %>% 
  cor () %>%  
  corrplot(method='color', diag=F, type='lower')

# find the distribution of score
df %>% ggplot(aes(MD.TOTAL))+geom_histogram(col="black", alpha=.5)

# correaltion of mood and ADHD self report
#df %>% ggplot(aes(y=ADHD.Total,x=Education))+geom_point() + scale_color_viridis_c()
# na.omit(df) %>% 
#   dplyr::select(MD.TOTAL, ADHD.Total,Race,Sex,Education,Age) %>% 
#   cor () #%>%  
  #corrplot(method='color', diag=F, type='lower')

```

## 4. Drug Use
no use-0, use-1, abuse-2, dependence-3
- participants were more likely to report dependence rather than use or abuse
- 131 participants reported either drug abuse or dependence


```{r}
# Specific drugs by Race and Sex
df %>% 
  filter(Race %in% c(1,2)) %>% 
  dplyr::select(Alcohol, THC,Cocaine,Stimulants, Sedative.hypnotics,Opioids, Race,Initial) %>% 
  pivot_longer(-c(Initial,Race)) %>% 
  ggplot(aes(x=name,fill=as.factor(value))) + 
  geom_bar(stat="count") + 
  scale_fill_viridis_d() + 
  theme_minimal() + 
  facet_wrap(~factor(Race))

df %>% 
  dplyr::select(Alcohol, THC,Cocaine,Stimulants, Sedative.hypnotics,Opioids, Sex,Initial) %>% 
  pivot_longer(-c(Initial,Sex)) %>% 
  ggplot(aes(x=name,fill=as.factor(value))) + 
  geom_bar(stat="count") + 
  scale_fill_viridis_d() + 
  theme_minimal() + 
  facet_wrap(~factor(Sex))

# New variables 
df$DRUG_dependence <- ifelse(df$THC==3 | df$Alcohol == 3 | df$Cocaine == 3 | df$Stimulants == 3 |         df$Sedative.hypnotics == 3 | df$Opioids == 3, 1, 0 )

df$DRUG_abuse <- ifelse(df$THC==2 | df$Alcohol == 2 | df$Cocaine == 2 | df$Stimulants == 2 | df$Sedative.hypnotics == 2 | df$Opioids == 2, 1, 0 )

df$DRUGS <- ifelse(df$DRUG_abuse ==1 | df$DRUG_dependence==1,1,0)

table(df$DRUG_dependence)
table(df$DRUG_abuse)
table(df$DRUGS)
```

## 6. Legal Issues
```{r}
df %>% 
  dplyr::select(Court.order, Disorderly.Conduct, Hx.of.Violence ,Initial) %>% 
  pivot_longer(-c(Initial)) %>% 
    ggplot(aes(x=value)) + 
  geom_bar(stat="count") + 
  facet_wrap(~name, scales="free") + 
  theme_minimal()

df$LEGAL_issues <- ifelse(df$Court.order==1 | df$Disorderly.Conduct==1 | df$Hx.of.Violence==1, 1,0)

df %>% 
  filter(Race %in% c(1,2)) %>% 
  ggplot(aes(x=factor(LEGAL_issues),fill=factor(Sex))) + 
    geom_bar(stat="count") + 
    scale_fill_viridis_d()+
    facet_wrap(~Race)

```

## 7. Abuse
```{r}
table(df$Abuse)
df %>% 
  ggplot(aes(Abuse)) + geom_bar(stat="count")

df$ABUSE <- ifelse(df$Abuse > 0, 1,0)
table(df$ABUSE)

df %>% 
  ggplot(aes(ABUSE, col=Sex)) + geom_bar(stat="count")
```

## 8. Medical Interventions

```{r}
df$MEDS <- ifelse(df$Non.subst.Dx >0 | df$Non.subst.Dx > 0, 0,1)
```

## 9. Suicide
- 49 participants attempted a suicide 
- seems to be association between mood disorder reporting and suicide, but not with ADHD 
- whites and females more likely to commit suicide
- suicide more likely in those less educated and young
- suicide rates increase as drug dependence increases 
- suicide rate increases substantially with any kind of abuse
- legal issues do not seem to predictive of suicide
- taking 2 or more subst. drugs might increases chances of suicide

```{r}
# how many attempted suicides were there
table(df$Suicide)

# assocation with mood disorder and adhd
df %>% ggplot(aes(x=ADHD.Total, y=Suicide)) + geom_point(position="jitter")
df %>% ggplot(aes(x=MD.TOTAL, y=Suicide)) + geom_point(position="jitter")

# suidide and demographics
df %>% 
  dplyr::select(Race, Sex, Initial,Suicide) %>% 
  pivot_longer(-c(Initial,Suicide)) %>%  
  ggplot(aes(value, fill=factor(Suicide))) + 
  geom_bar(position="dodge") + 
  facet_wrap(~name, scales="free")

df %>% ggplot(aes(x=Education, y=Suicide)) + geom_point(position="jitter")
df %>% ggplot(aes(x=Age, y=Suicide)) + geom_point(position="jitter")

# Drug use 
df %>% 
  dplyr::select(Alcohol, THC,Cocaine,Stimulants, Sedative.hypnotics,Opioids,Suicide) %>% 
  pivot_longer(-c(Suicide)) %>%  
  ggplot(aes(value, fill=factor(Suicide))) + 
  geom_bar(position="dodge") + 
  facet_wrap(~name, scales="free")

# Abuse
df %>% ggplot(aes(Abuse, fill=factor(Suicide))) + geom_bar(stat="count",position = "dodge") 
names(df)

# Legal issues 
df %>% 
  dplyr::select(Court.order, Hx.of.Violence, Disorderly.Conduct, LEGAL_issues ,Suicide) %>% 
  pivot_longer(-c(Suicide)) %>%  
  ggplot(aes(factor(value), fill=factor(Suicide))) + 
  geom_bar(position="dodge") + 
  facet_wrap(~name, scales="free")

# medications
df %>% 
  dplyr::select(Non.subst.Dx, Subst.Dx, Psych.meds., MEDS ,Suicide) %>% 
  pivot_longer(-c(Suicide)) %>%  
  ggplot(aes(factor(value), fill=factor(Suicide))) + 
  geom_bar(position="dodge") + 
  facet_wrap(~name, scales="free")
```

# Unsupervised Learning 

## Clustering 
*Use a clustering method to find clusters of patients here. Whether you choose to use k-means clustering or hierarchical clustering is up to you as long as you reason through your work. You are free to be creative in terms of which variables or some combination of those you want to use. Can you come up with creative names for the profiles you found? (40 points)*


kmeans= within cluster variation is small
Within Cluster Variation via squared distance using euclidean 
solution is dependant on the starting point 

hierarchical - do not have to choose k, gives all k
bottom up or top down 
dendogram
choose linkage (distance between clusters): complete, single, centroid, average... complete and average are the most common
euclidean distance vs correlation based distance 


### Preprocessing for Clustering 
https://medium.com/@evgen.ryzhkov/5-stages-of-data-preprocessing-for-k-means-clustering-b755426f9932
*Requirements*
1. Numerical Only: 
2. No outliers
3. symmertical Distribution of values
4. variables on the same scale:
normalize from 0 to 1
5. no coliniearity: 
check correlation matrix before clustering 
6. Few number of dimensions: 

new dataframe to store the dimension reduction dataframe that will be used for clustering
For purposes of preparing the dataset for clustering analysis, the main objective was to reduce the dimensionality of the raw dataset. The raw dataset contains 55 variables, but many of these fall into the following categories

For each of the following categories,a single predictor value will be estimated. This will reduce the number of variables from 55 to 10, and will allow for a more interpretable clustering to be performed. The varaibles will be scaled from 0-1 and analyzed so that infromation loss is kept to a minimum when combining and scaling.  

For variables that are binary: 
- make sure scales are from 0 to 1 
```{r}
# clustering dataframe 
df_clust_raw <-  df %>% 
       dplyr::select(Age,
                     Sex,
                     Race,
                     Education,        
                     ADHD.Total,
                     MD.TOTAL,
                     LEGAL_issues,         
                     DRUGS,
                     ABUSE,
                     MEDS,
                     Suicide
                     )


# impute missing values
impute_temp <- mice(df_clust_raw,
                  m = 5,
                  method = "pmm",
                  maxit = 5,
                  seed = 2021,
                  )
df_clust_raw_impute <- complete(impute_temp)

# center and scale
temp <- preProcess(x = df_clust_raw_impute,
                       method = c("center", "scale"))
  

df_clust_raw_scaled <- predict(temp, df_clust_raw_impute)

# make dummies

```


### Make clusters
REF: https://www.statmethods.net/advstats/cluster.html
- 1 hierarchical, 1 kmeans
- 
```{r}
library(pvclust)
library(ggdendro)
# 1. hierarchical cluster, pre-processed
###"The pvclust( ) function in the pvclust package provides p-values for hierarchical clustering based on multiscale bootstrap resampling. Clusters that are highly supported by the data will have large p values. Interpretation details are provided Suzuki. Be aware that pvclust clusters columns, not rows. Transpose your data before using."

# calculate matrix of distances between variables 
distances <- dist(df_clust_raw_scaled)

# using hclust
hclust <- hclust(distances, method="ward.D")
plot(hclust)
ggdendrogram(as.dendrogram(hclust))

# using pvclust
pvclust <-  pvclust(t(df_clust_raw_impute), method="ward.D", method.dist="euclidean")
plot(pvclust)
# add rectangles around groups highly supported by the data
pvrect(pvclust, alpha=.95)



clust4 <- cutree(hclust, 4)

# assign labels to initial data\
df_clust_raw_impute$label <- clust4

#df_clust_raw_impute$kclust <- kclust5$cluster

```

# 2. kmeans, pre-processed
```{r}
## determine number of clusters
# Determine number of clusters
# wss <- (nrow(df_clust)-1)*sum(apply(df_clust,2,var))
# for (i in 2:15) wss[i] <- sum(kmeans(df_clust,
#    centers=i)$withinss)
# #The analyst looks for a bend in the plot similar to a scree test in factor analysis. See Everitt & Hothorn (pg. 251).
# plot(1:15, wss, type="b", xlab="Number of Clusters",
#   ylab="Within groups sum of squares")
# 
# kclust5 <- kmeans(df_clust_raw_scaled, 5)
# kclust4 <- kmeans(df_clust_raw_scaled, 4)
# kclust3 <- kmeans(df_clust_raw_scaled, 3)
# kclust6 <- kmeans(df_clust_raw_scaled, 6)
# kclust7 <- kmeans(df_clust_raw_scaled, 7)
# # we want the lowest total withn cluster sum of square 
# kclust7$withinss %>% sum
# kclust4$withinss %>% sum
# kclust5$withinss %>% sum
# kclust6$withinss %>% sum
# Solution to issue 1: Compute k-means for a range of k values, for example by varying k between 2 and 10. Then, choose the best k by comparing the clustering results obtained for the different k values.
# Solution to issue 2: Compute K-means algorithm several times with different initial cluster centers. The run with the lowest total within-cluster sum of square is selected as the final clustering solution.
# To avoid distortions caused by excessive outliers, it’s possible to use PAM algorithm, which is less sensitive to outliers.
```

### Visualize Clusters 
https://www.statmethods.net/advstats/cluster.html
- Look at groups

Clust1: No drug use or medication, or legal issues
Clust2: High Abuse, most likely to attempt suicide, generally female
Clust3: Low ADHD and mood disorder self report, not likely to attempt suicide
Clust4: Male,  No Abuse 
```{r}

table <- df_clust_raw_impute %>%
  pivot_longer(-c(label)) %>% 
  group_by(label, name) %>%
  summarize(mean =mean(na.omit(value))) %>% 
  arrange(name)

table %>%
  ggplot(aes(y=mean,  x=label)) + geom_bar(stat="identity", position='dodge') + facet_wrap(~name, scales="free")
# aggregate(df_clust_raw_impute, by=list(cluster=kclust5$cluster), na.omit(mean))
```

```{r}

df_unscaled %>% 
  pivot_longer(-label) %>% 
  group_by(label,name) %>% 
  summarize(mean=mean(value)) %>% 
  ggplot(aes(x=name,y=mean,fill=label)) + 
  geom_bar(stat="identity",position="stack") + 
  scale_fill_viridis_c()
# sex

df_unscaled %>% pivot_longer(1:9) %>%  ggplot(aes(x=name,y=value, fill=factor(label))) + geom_boxplot() 



df_unscaled %>% 
  pivot_longer(-label) %>% 
  group_by(label,name) %>% 
  summarize(mean=mean(na.omit(value))) %>% 
  ggplot(aes(x=label, y=mean)) + geom_bar(stat="identity",position = "dodge") + facet_wrap(~name, nrow=3, scales="free")


# heat map without the individual questions just the totals
```
## PCA
Let’s explore using Principal Component Analysis on this dataset. You will note that there are
different types of questions in the dataset: column: E-W: ADHD self-report; column X – AM:
mood disorders questionnaire, column AN-AS: Individual Substance Misuse; etc. You could just
use ONE of the sets of questionnaire, for example, you can conduct PCA on the ADHD score, or
mood disorder score, etc. Please reason through your work as you decide on which sets of
variables you want to use to conduct Principal Component Analysis. What did you learn from the
PCA? Can you comment on which question may have a heavy bearing on the score? (40 points)

```{r}


```


# Supervised Learning
could find clusters or principles componants that are correlated with the target 

## train/test split
```{r}
library(caret)
set.seed(2021)
trainIndex <- createDataPartition(processed$Loan_Status, p = .8) %>% unlist()
training <- processed[ trainIndex,]
testing  <- processed[-trainIndex,]
```

## Gradient Boosting
Assume you are modeling whether a patient attempted suicide (column AX). This is a binary
target variable. Please use Gradient Boosting to predict whether a patient attempts suicides.
Please use whatever boosting approach you deem appropriate. But please be sure to walk us
through your steps. (50 points)

### GBM data setup
use the same setup as the clustered dataframe 
create dummy variables 
```{r}
df_boost <- df_clust_raw_impute
binary_vars <- c('Race','Sex', 'LEGAL_issues', 'DRUGS', 'ABUSE', 'MEDS','label' )
df_boost <- update_columns(df_boost, binary_vars, as.factor)
df_boost <- df_boost %>% dummify(binary_vars)
ind <- nzv(df_boost)
df_boost <- df_boost[,-ind]
df_boost$Suicide <- as.factor(df$Suicide)
levels(df_boost$Suicide)

# remove obs where Suicide is 0
df_boost <- df_boost %>% filter(complete.cases(.))
```

### Train/Test Split

It is desirable to have a holdout dataset to evaluate the models.  Below, 20% of the dataset is split off into a test set, while the remaining 80% becomes the training set (the models are trained on this).  This is done for both datasets

```{r}
set.seed(2021)
trainIndex <- createDataPartition(df_boost$Suicide, p = .8) %>% unlist()
training <- df_boost[ trainIndex,]
testing  <- df_boost[-trainIndex,]
```

### Cross Validation Setup

Ten-fold cross validation is used as an aid in model training; this is seperate from the holdout testing set, which will be used solely for model evaluation at the end of training.

```{r}
# 10 fold cv
ctrl <- trainControl(method="repeatedcv",
                     number=10)
              
```

### Build GBM model 
```{r}
# hyperparamer grid
set.seed(2021)
grid = expand.grid(.n.trees = seq(6000, 15000, by=3000),
                   .interaction.depth=seq(6,26,4),
                   .shrinkage=c(0.01, 0.001,0.0001),
                   .n.minobsinnode=c(1))

gbm.mod <- train(Suicide ~ ., data=training, 
                  method="gbm",
                  metric="Accuracy",
                  #preProc=c("center","scale", "bagImpute"),  
                  tuneGrid = grid,
                  trControl=ctrl,
                 verbose=F)

plot(df_boost)
```

### Evaluate gbm
```{r}
gbm.predict <- predict(gbm.mod, testing)
gbm.mod$finalModel
plot(gbm.mod)
confusionMatrix(gbm.predict, testing$Suicide)

```

##  Support Vector Machine
Using the same target variable (suicide attempt), please use support vector machine to model this. You might want to consider reducing the number of variables or somehow use extracted
information from the variables. This can be a really fun modeling task! (50 points)

```{r}


```


